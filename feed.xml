<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://mehmetemreakbulut.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://mehmetemreakbulut.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-10-19T13:01:33+00:00</updated><id>https://mehmetemreakbulut.github.io/feed.xml</id><title type="html">Mehmet Emre Akbulut</title><subtitle>Mehmet Emre Akbulut&apos;s personal website </subtitle><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://mehmetemreakbulut.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://mehmetemreakbulut.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://mehmetemreakbulut.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">a post with tabs</title><link href="https://mehmetemreakbulut.github.io/blog/2024/tabs/" rel="alternate" type="text/html" title="a post with tabs"/><published>2024-05-01T00:32:13+00:00</published><updated>2024-05-01T00:32:13+00:00</updated><id>https://mehmetemreakbulut.github.io/blog/2024/tabs</id><content type="html" xml:base="https://mehmetemreakbulut.github.io/blog/2024/tabs/"><![CDATA[<hr/> <h4 id="distilled-images-by-genie-without-any-image-prior-loss1">Distilled images by GENIE (without any image prior loss) [1]</h4> <p>GENIE: Synthetic Data for Zero-Shot Quantization Neural Network Quantization is one of the remarkable areas in the field of machine learning for the recent years. As the machine learning models become bigger and bigger, many challenges arise when deploying the mocels into resource-constrained devices. Quantization reduces the bit representation of the model parameters into lower precisions so that the deployed model fulfills the memory and computation requirements of the hardware. Prior works show that different quantization algorithms enable us to deploy models that have significantly lower memory consumption and fewer arithmetic operations with little loss in task performance. Quantization-Aware Training (QAT) provides researchers a scheme to train the neural network in a simulated way so that it learns the parameters priorly by being aware of future quantization step. However, data is not always available in some EdgeAI use cases because of the privacy and security concerns. Post-Training Quantization (PTQ) offers some solutions in order to compress model with little or no data in a quite short time but less accuracy comparing to the QAT. Zero-Shot Quantization synthesizes data to be used in model without real data. ZSQ generates elaborate replicas to ensure the distribution of intermediate feature maps matches the statistics of the batch normalization layers [1].</p> <hr/> <p>GENIE is framework proposed by Samsung Research which focuses on generating proper synthetic data for the quantization process. We can summarizes their contributions: By combining generationg and distillation, proposing a scheme for data synthesizing. Swing convolution: substitution for convolution with stride n &gt; 1 A new quantization scheme that is submodule of GENIE</p> <hr/> <p>Data Distillation GENIE-D To synthesize data, Zero-Shot Quantization uses batch normalization layers statistics of pre-trained model (mean μ and standard deviation σ of). Latent vectors from a Gaussian distribution is used by Generator Based Approaches as input of the generator. The generator is meant to acquire common knowledge about the input domain. Quantized networks, particularly with QAT, are limited by the redundant information required for quantization, despite the potential for endless data generation of the GBA approach. In contrast, Distill-Based Approach propagates error to images so it converges quickly to little loss. In this study, researchers create a generator that generates synthetic data while distilling the information into latent vectors z from a normal distribution. In other words, the synthetic images are indirectly distilled by the latent vectors, which are trained and updated with each iteration. Quantization Algorithm GENIE-M AdaRound [2] proposed a rounding scheme that allocated weights to one of the two nearest quantization points. However as authors stated, “It is non-trivial to combine the two tasks: any change in the step size would result in a different quadratic unconstrained binary optimization (QUBO) problem”. Optimizing the step size s leads to the change in the base B, it can cause a conflict with the softbits V to be optimized. GENIE propose a joint optimization algorithm. They detach B from s and their mutual dependency. In other words, they learn s independent of B.</p> <hr/> <p>Results Performance: The fark data distilled by GENIE is more informative than the SotA zero shot quantization frameworks in the literature. Even with 256 images GENIE gives better results than Qimera and ZeroQ. The figures taken from [3], [4] and [5]On Real Data: GENIE algorithm is tested on various CNN models by using a randomly sampled data from ImageNet. GENIE outperforms the other by using a joint optimization technique. Results on Real Data [1]References Jeon, Yongkweon et al. “Genie: Show Me the Data for Quantization.” 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2022): 12064–12073.</p> <ol> <li>Nagel, Markus et al. “Up or Down? Adaptive Rounding for Post-Training Quantization.” ArXiv abs/2004.10568 (2020): n. pag.</li> <li>Choi, Kanghyun et al. “It’s All In the Teacher: Zero-Shot Quantization Brought Closer to the Teacher.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2022): 8301–8311.</li> <li>Liu, Yuang et al. “Zero-shot Adversarial Quantization.” 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021): 1512–1521.</li> <li>Zhong, Yunshan et al. “IntraQ: Learning Synthetic Images with Intra-Class Heterogeneity for Zero-Shot Network Quantization.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021): 12329–12338.</li> </ol>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included tabs in a post could look like]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://mehmetemreakbulut.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://mehmetemreakbulut.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://mehmetemreakbulut.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry></feed>